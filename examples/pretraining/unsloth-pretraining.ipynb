{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Installation"]},{"cell_type":"markdown","metadata":{},"source":["## Google Colab"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%capture\n","# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n","!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n","!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"]},{"cell_type":"markdown","metadata":{},"source":["## Kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# %%capture\n","# !mamba install --force-reinstall aiohttp -y\n","# !pip install -U \"xformers<0.0.26\" --index-url https://download.pytorch.org/whl/cu121\n","# !pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n","\n","# # Temporary fix for https://github.com/huggingface/datasets/issues/6753\n","# !pip install datasets==2.16.0 fsspec==2023.10.0 gcsfs==2023.10.0\n","\n","# import os\n","# os.environ[\"WANDB_DISABLED\"] = \"true\""]},{"cell_type":"markdown","metadata":{},"source":["# Prepare model"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T07:49:06.755189Z","iopub.status.busy":"2024-07-20T07:49:06.754830Z","iopub.status.idle":"2024-07-20T07:49:48.498209Z","shell.execute_reply":"2024-07-20T07:49:48.497071Z","shell.execute_reply.started":"2024-07-20T07:49:06.755158Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"]},{"name":"stderr","output_type":"stream","text":["2024-07-20 07:49:17.955124: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-20 07:49:17.955182: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-20 07:49:17.959708: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"name":"stdout","output_type":"stream","text":["==((====))==  Unsloth: Fast Qwen2 patching release 2024.7\n","   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.2.2+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.25.post1. FA2 = False]\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"name":"stderr","output_type":"stream","text":["vietgpt/sailor-1.8B does not have a padding token! Will use pad_token = <|PAD_TOKEN|>.\n"]}],"source":["from unsloth import FastLanguageModel\n","import torch\n","max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n","dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n","load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n","\n","# CÃ¡c model há»— trá»£: Llama, Qwen, Phi, Gemma, Mistral, TinyLlama\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = \"vietgpt/sailor-1.8B\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n","    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T07:49:48.501030Z","iopub.status.busy":"2024-07-20T07:49:48.500254Z","iopub.status.idle":"2024-07-20T07:49:48.507633Z","shell.execute_reply":"2024-07-20T07:49:48.506663Z","shell.execute_reply.started":"2024-07-20T07:49:48.500993Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Qwen2ForCausalLM(\n","  (model): Qwen2Model(\n","    (embed_tokens): Embedding(151646, 2048)\n","    (layers): ModuleList(\n","      (0-23): 24 x Qwen2DecoderLayer(\n","        (self_attn): Qwen2Attention(\n","          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n","          (k_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n","          (v_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n","          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): Qwen2MLP(\n","          (gate_proj): Linear4bit(in_features=2048, out_features=5504, bias=False)\n","          (up_proj): Linear4bit(in_features=2048, out_features=5504, bias=False)\n","          (down_proj): Linear4bit(in_features=5504, out_features=2048, bias=False)\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): Qwen2RMSNorm()\n","        (post_attention_layernorm): Qwen2RMSNorm()\n","      )\n","    )\n","    (norm): Qwen2RMSNorm()\n","  )\n","  (lm_head): Linear(in_features=2048, out_features=151646, bias=False)\n",")\n"]}],"source":["print(model)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T07:49:48.509484Z","iopub.status.busy":"2024-07-20T07:49:48.509062Z","iopub.status.idle":"2024-07-20T07:49:59.824447Z","shell.execute_reply":"2024-07-20T07:49:59.823635Z","shell.execute_reply.started":"2024-07-20T07:49:48.509444Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Unsloth: Offloading input_embeddings to disk to save VRAM\n","Unsloth: Offloading output_embeddings to disk to save VRAM\n"]},{"name":"stderr","output_type":"stream","text":["Unsloth 2024.7 patched 24 layers with 0 QKV layers, 24 O layers and 24 MLP layers.\n"]},{"name":"stdout","output_type":"stream","text":["Unsloth: Casting embed_tokens to float32\n","Unsloth: Casting lm_head to float32\n"]}],"source":["model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\",\n","\n","                      \"embed_tokens\", \"lm_head\",], # Add for continual pretraining\n","    lora_alpha = 32,\n","    lora_dropout = 0, # Supports any, but = 0 is optimized\n","    bias = \"none\",    # Supports any, but = \"none\" is optimized\n","    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n","    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n","    random_state = 3407,\n","    use_rslora = True,   # We support rank stabilized LoRA\n","    loftq_config = None, # And LoftQ\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Prepare Dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T07:49:59.825846Z","iopub.status.busy":"2024-07-20T07:49:59.825558Z","iopub.status.idle":"2024-07-20T07:49:59.830560Z","shell.execute_reply":"2024-07-20T07:49:59.829664Z","shell.execute_reply.started":"2024-07-20T07:49:59.825824Z"},"trusted":true},"outputs":[],"source":["EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n","def formatting_prompts_func(examples):\n","    return { \"text\" : examples[\"text\"] + EOS_TOKEN, }\n","pass"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T07:49:59.834079Z","iopub.status.busy":"2024-07-20T07:49:59.833514Z","iopub.status.idle":"2024-07-20T07:50:01.263128Z","shell.execute_reply":"2024-07-20T07:50:01.262203Z","shell.execute_reply.started":"2024-07-20T07:49:59.834051Z"},"trusted":true},"outputs":[],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"iamnguyen/food_content\")\n","dataset = dataset.map(formatting_prompts_func, num_proc=4,)"]},{"cell_type":"markdown","metadata":{},"source":["# Training model"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T07:50:01.436016Z","iopub.status.busy":"2024-07-20T07:50:01.435652Z","iopub.status.idle":"2024-07-20T07:50:04.333959Z","shell.execute_reply":"2024-07-20T07:50:04.332759Z","shell.execute_reply.started":"2024-07-20T07:50:01.435982Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid, fd = os.forkpty()\n"]},{"name":"stdout","output_type":"stream","text":["Token is valid (permission: write).\n","Your token has been saved in your configured git credential helpers (store).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["!git config --global credential.helper store\n","!huggingface-cli login --add-to-git-credential --token hf_..."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T07:50:04.336030Z","iopub.status.busy":"2024-07-20T07:50:04.335678Z","iopub.status.idle":"2024-07-20T07:50:39.355172Z","shell.execute_reply":"2024-07-20T07:50:39.354096Z","shell.execute_reply.started":"2024-07-20T07:50:04.336002Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bfb18956652a4cfba0c6193e27ccffe5","version_major":2,"version_minor":0},"text/plain":["Map (num_proc=4):   0%|          | 0/1113 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import TrainingArguments\n","from unsloth import is_bfloat16_supported\n","from unsloth import UnslothTrainer, UnslothTrainingArguments\n","\n","trainer = UnslothTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = dataset['train'],\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 4,\n","\n","    args = UnslothTrainingArguments(\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 32,\n","\n","        # Use warmup_ratio and num_train_epochs for longer runs!\n","        warmup_steps = 10,\n","        num_train_epochs = 3,\n","\n","        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n","        learning_rate = 5e-5,\n","        embedding_learning_rate = 1e-5,\n","\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        logging_steps = 1,\n","        optim = \"paged_adamw_8bit\",\n","        weight_decay = 0.01,\n","        lr_scheduler_type = \"cosine\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","        \n","        report_to='none',\n","        save_strategy = 'steps',\n","        save_steps = 4,\n","        save_total_limit = 1,\n","        push_to_hub = True,\n","        hub_strategy = 'checkpoint',\n","        hub_model_id = 'nlplabtdtu/Sailor-food'\n","    ),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-20T07:50:39.357481Z","iopub.status.busy":"2024-07-20T07:50:39.356725Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Unsloth: Setting lr = 1.00e-05 instead of 5.00e-05 for embed_tokens.\n","Unsloth: Setting lr = 1.00e-05 instead of 5.00e-05 for lm_head.\n"]},{"name":"stderr","output_type":"stream","text":["==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n","   \\\\   /|    Num examples = 1,113 | Num Epochs = 3\n","O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 32\n","\\        /    Total batch size = 64 | Total steps = 51\n"," \"-____-\"     Number of trainable parameters = 741,072,896\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='9' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 9/51 06:16 < 37:39, 0.02 it/s, Epoch 0.46/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>2.303700</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>2.333700</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>2.271500</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>2.242200</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>2.207900</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>2.223100</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>2.285500</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["trainer_stats = trainer.train()\n","# trainer_stats = trainer.train(resume_from_checkpoint='last-checkpoint')"]},{"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# alpaca_prompt = Copied from above\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","inputs = tokenizer(\"Thá»‹t cua lÃ  pháº§n thá»‹t Ä‘Æ°á»£c láº¥y tá»« pháº§n thÃ¢n vÃ  pháº§n cÃ ng\", \n","                   return_tensors = \"pt\").to(\"cuda\")\n","\n","outputs = model.generate(**inputs, max_new_tokens = 128, use_cache = True)\n","tokenizer.batch_decode(outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Inference with Unsloth\n","from unsloth import FastLanguageModel\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n",")\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","\n","text = \"\"\n","inputs = tokenizer(text, return_tensors = \"pt\").to(\"cuda\")\n","\n","outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n","tokenizer.batch_decode(outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Inference with Peft\n","from peft import PeftConfig, PeftModel\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","\n","config = PeftConfig.from_pretrained(\"nlplabtdtu/vilaw-qwen-raft\")\n","model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, device_map=\"auto\").eval()\n","# model = AutoModelForCausalLM.from_pretrained('nlplabtdtu/vilaw-qwen-1.5b-instruct', device_map=\"auto\").eval()\n","tokenizer = AutoTokenizer.from_pretrained(\"nlplabtdtu/vilaw-qwen-raft\")\n","\n","model = PeftModel.from_pretrained(model, \"nlplabtdtu/vilaw-qwen-raft\", adapter_name=\"adapter1\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
